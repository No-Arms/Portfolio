{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Za7lau_enfci"
   },
   "source": [
    "# Neural Networks\n",
    "Noah Armsworthy\n",
    "September 2nd, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8NISnUkDxJI"
   },
   "source": [
    "We are not going to use Keras to build the network, but we will use their module to import our dataset. https://www.digitalocean.com/community/tutorials/mnist-dataset-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "388qAKAfELQW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTuptyEAD63Q",
    "outputId": "8c98cc3a-cc3f-4f18-aa7d-4288a2432dd7"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ss98SEBLEFYF",
    "outputId": "397443ef-b658-4f02-9f1e-9877929de059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "h1CXqo6xEJW_",
    "outputId": "5d2c6185-c68b-4caa-fce8-25f7e949bcba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target value: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "print('Target value:', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXeXQShxndZC"
   },
   "source": [
    "##Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RqFQPjG-nxOg"
   },
   "outputs": [],
   "source": [
    "'''Abstract method for activations to maintain structure signature \n",
    "and allow for easy forward and backward calls'''\n",
    "import abc\n",
    "\n",
    "class Activations(metaclass=abc.ABCMeta):\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def backward(self, grad, original_input):\n",
    "        pass\n",
    "\n",
    "#Relu https://deepai.org/machine-learning-glossary-and-terms/relu\n",
    "class ReLU(Activations):\n",
    "    def forward(self, x):\n",
    "        return np.maximum(0.0,x)\n",
    "\n",
    "    def backward(self, grad, original_input):\n",
    "    x = original_input\n",
    "        return grad * (x>0.0)\n",
    "\n",
    "    #Allows for easy calling forward using only ReLU(x)\n",
    "    def __call__(self, x, mode=None):\n",
    "        return self.forward(x)\n",
    "\n",
    "#Need to test\n",
    "#Sigmoid https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n",
    "class Sigmoid(Activations):\n",
    "    def forward(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, grad, original_input):\n",
    "        x = self.forward(original_input)\n",
    "        return grad * ( x * (1 - x))\n",
    "    \n",
    "    def __call__(self, x, mode=None):\n",
    "        return self.forward(x)\n",
    "#Tanh https://blogs.cuit.columbia.edu/zp2130/derivative_of_tanh_function/\n",
    "class Tanh(Activations):\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def backward(self, grad, original_input):\n",
    "        return grad * (1 - np.tanh(original_input)**2)\n",
    "    \n",
    "    def __call__(self, x, mode=None):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybOOlqV2Xiwk"
   },
   "source": [
    "##SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9OobnbxTDKqg"
   },
   "outputs": [],
   "source": [
    "#Soft max takes in a sample of values and scales them to\n",
    "#Values between 0 and 1, making them interpretable as probabilities\n",
    "def softMax(x):\n",
    "  #Currently made to accept an np array of arrays to process mutliple samples\n",
    "  return np.exp(x)/np.sum(np.exp(x), axis=1, keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyLY3IIdCUHP",
    "outputId": "389a7458-0d8e-48fd-fc75-a47c2820e80a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result after a ReLU:\n",
      " [[ 2.   0.8]\n",
      " [ 0.  10. ]]\n",
      "Shifted:\n",
      " [[  0.   -1.2]\n",
      " [-10.    0. ]]\n",
      "SoftMax returns:\n",
      " [[7.68524783e-01 2.31475217e-01]\n",
      " [4.53978687e-05 9.99954602e-01]]  \n",
      " Each row (sample) sums up to:\n",
      " [[1.]\n",
      " [1.]] AKA 100%\n",
      "Shifted Values don't change the result of softMax:\n",
      " [[7.68524783e-01 2.31475217e-01]\n",
      " [4.53978687e-05 9.99954602e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "x = np.array([[2.0,   0.8], \n",
    "              [-0.4,  10.0]])\n",
    "\n",
    "#ReLU will convert negative values to 0.0\n",
    "relu = ReLU()\n",
    "relX = relu(x)\n",
    "print('Result after a ReLU:\\n', relX)\n",
    "\n",
    "#While implementing softmax, you can shift your input. \n",
    "#This does not affect the result of the softmax operation and improves numerical stability.\n",
    "shiftedX = relX - np.max(relX, axis=1, keepdims=True)\n",
    "print('Shifted:\\n', shiftedX)\n",
    "sM = softMax(relX)\n",
    "print('SoftMax returns:\\n',sM, ' \\n Each row (sample) sums up to:\\n', np.sum(sM, axis=1, keepdims=True), 'AKA 100%')\n",
    "sM = softMax(shiftedX)\n",
    "print('Shifted Values don\\'t change the result of softMax:\\n',sM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IyREPk11TeQq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Old for when we hand in a sample at a time\\nfor sample in relX:\\n  sM = softMax(sample)\\n  print('Normal:', sM, ' Sums to:', np.sum(sM))\\n  print('Shifted:',softMax(sample - np.max(sample)))\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Old for when we hand in a sample at a time\n",
    "for sample in relX:\n",
    "  sM = softMax(sample)\n",
    "  print('Normal:', sM, ' Sums to:', np.sum(sM))\n",
    "  print('Shifted:',softMax(sample - np.max(sample)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6MvVdxiXlgh"
   },
   "source": [
    "##Cross Entropy Loss\n",
    "Takes in probabilities from the softmax. CE will measure the distance from the target values.\n",
    "np.log uses the natural logarithm of base $e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBGTq0A-c1WX"
   },
   "source": [
    "###Entropy \n",
    "measured randomness\n",
    "(higher = less certainty of the pick)\n",
    "https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Yy11kq68XoSz"
   },
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "  return -np.sum(x * np.log(x), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f96RdrYboAl6",
    "outputId": "1e478f09-f910-4ea6-fda5-918e51dd4796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy value between 1.0% and 99.0% : 0.056001534354847345\n",
      "Entropy value between 20.0% and 80.0% : 0.5004024235381879\n",
      "Entropy value between 50.0% and 50.0% : 0.6931471805599453\n",
      "Entropy value between 4 options, each with 25% likelihood : 1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "'''When there is a more obviously occuring winner\n",
    "    ie. 10% vs 90%, entropy will be low because it's likely to choose the 90% term.\n",
    "    However, in a 50/50 split, entropy is high because it is hard to guess which option will be selected. (Greater uncertainty)\n",
    "'''\n",
    "a = [[0.01, 0.99],[0.2, 0.8], [0.5, 0.5]]\n",
    "entropyA = entropy(a)\n",
    "for i in range(0, len(a)):\n",
    "  print(f\"Entropy value between {a[i][0]*100}% and {a[i][1]*100}% : {entropyA[i][0]}\")\n",
    "\n",
    "#With more options with relatively even spread, we can see even higher entropy values\n",
    "print(f\"Entropy value between 4 options, each with 25% likelihood : {entropy([[0.25, 0.25, 0.25, 0.25]])[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4PjBz8gnvG7",
    "outputId": "c7bc2cf3-4079-40a3-cc3b-476629f2835d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned probabilities from above:\n",
      "[76.8525% , 23.1475%]\n",
      "[0.0045% , 99.9955%]\n",
      "Entropy by row (sample):\n",
      " [[0.541053]\n",
      " [0.000499]]\n"
     ]
    }
   ],
   "source": [
    "#Our values from above\n",
    "print('Returned probabilities from above:')\n",
    "for row in sM:\n",
    "  print(\"[{:.4f}% , {:.4f}%]\".format(row[0]*100, row[1]*100))\n",
    "print('Entropy by row (sample):\\n', np.array_str(entropy(sM), precision=6, suppress_small=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvzF3zrBd2s4",
    "outputId": "1c2ca29a-c3e4-4d3d-ced3-b6f4b15e6842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock one-hot targets (1 represents the correct selection):\n",
      " [[0 1]\n",
      " [1 0]]\n",
      "When log probabilities are multiplied by the one-hot targets we get\n",
      " [[ 0.          1.46328247]\n",
      " [10.0000454   0.        ]]\n",
      "Which means the loss per sample is:\n",
      " [[ 1.4633]\n",
      " [10.    ]]\n"
     ]
    }
   ],
   "source": [
    "def entropyLoss(x, t):\n",
    "  return -np.sum(t * np.log(sM),  axis=1, keepdims=True)\n",
    "\n",
    "#Mock targets in that are one-hot encoded\n",
    "tWrong = np.array([[0,1], [1,0]])\n",
    "print('Mock one-hot targets (1 represents the correct selection):\\n', tWrong)\n",
    "print('When log probabilities are multiplied by the one-hot targets we get\\n', tWrong * -np.log(sM))\n",
    "print('Which means the loss per sample is:\\n', np.array_str(entropyLoss(sM, tWrong), precision=4, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2V0p6ZEhuPVj"
   },
   "source": [
    "For both samples, we set the target as the value with the lowest percentage of likelihood, meaning the most incorrect choice it could be. The difference in the loss values is a degree of how wrong we were based on what was expected from the percentages above.\n",
    "\n",
    "If we were to have targets align with what our data says it more likely, we would see low loss numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jiZLxEWzuxz8",
    "outputId": "6b3ef202-7aa1-444c-d578-98d965eddc63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target aligning with highest percentage means the loss per sample is:\n",
      " [[0.2632825]\n",
      " [0.0000454]]\n"
     ]
    }
   ],
   "source": [
    "tRight = np.array([[1,0], [0,1]])\n",
    "print('Target aligning with highest percentage means the loss per sample is:\\n', np.array_str(entropyLoss(sM, tRight), precision=7, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WIP ^^ needs explanation on 'cross' part of cross entropy*\n",
    "Probably need to go over the backwards pass before this too. cri\n",
    "\n",
    "Now we can group these functions into our activation structure and test that it is indeed returning what's expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.   0.8]\n",
      " [-0.4 10. ]]\n",
      "No targets given(Probabilities): \n",
      " [[0.7685248 0.2314752]\n",
      " [0.0000304 0.9999696]]\n",
      "Targets given(Softmax Cross E Loss): \n",
      " [ 1.46328247 10.40003043]\n"
     ]
    }
   ],
   "source": [
    "'''https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html'''\n",
    "class SoftMaxCrossLoss(Activations):\n",
    "    def __init__(self):\n",
    "        self.probs = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        #y = none means we are in testing mode, there is no target\n",
    "        # Having a y would mean we are in training mode\n",
    "        \n",
    "        #Shifting our x values for numerical stability\n",
    "        shifted_x = x - np.max(x, axis=1, keepdims=True)\n",
    "        \n",
    "        #Storing the probabilities\n",
    "        self.probs = np.exp(shifted_x) / np.sum(np.exp(shifted_x), axis=1, keepdims=True)\n",
    "        #print('Probabilities of the samples : ', np.array_str(self.probs, precision=7, suppress_small=True))\n",
    "        \n",
    "        if y is None:\n",
    "            #print('I know Im testing, this should be probs:')\n",
    "            #Here we are simply returning probabilities to make an prediction \n",
    "            return self.probs\n",
    "        else:\n",
    "            #print('Im returning loss now')\n",
    "            self.y = y\n",
    "            #Returning the total loss of each sample (row) by summing the columns\n",
    "            return -1 * np.sum(self.y * np.log(self.probs), axis=1)\n",
    "        \n",
    "    def backward(self, grad, original_input):\n",
    "        return (self.probs - self.y) * grad\n",
    "    \n",
    "    def __call__(self, x, y=None, mode=None):\n",
    "        return self.forward(x,y)\n",
    "    \n",
    "print(x)\n",
    "softMCE = SoftMaxCrossLoss()\n",
    "#Not giving targets will return probabilities\n",
    "print('No targets given(Probabilities): \\n',np.array_str(softMCE(x), precision=7, suppress_small=True))\n",
    "print('Targets given(Softmax Cross E Loss): \\n',softMCE(x, y=tWrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Layers class for the layers of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers(metaclass=abc.ABCMeta):\n",
    "    @abc.abstractmethod\n",
    "    def forward(self,x):\n",
    "        return\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def backward(self, grad, original_input):\n",
    "        return\n",
    "\n",
    "'''Fully Connected Layer. Linear combination of the inputs: w0x0 + w1x1 + ...+ wnxn + b for n number of weights, and b bias.'''\n",
    "class AffineLayer(Layers):\n",
    "    def __init__(self, input_dim, hidden_units):\n",
    "        ran = np.sqrt(1/input_dim)\n",
    "        \n",
    "                #Initializing the weights and bias, in that order\n",
    "        self.parameters = [np.random.uniform(-ran,ran,(hidden_units,input_dim)),\n",
    "                           np.random.uniform(-ran,ran,hidden_units)]\n",
    "        \n",
    "        self.grads = [np.zeros_like(self.parameters[0]),np.zeros_like(self.parameters[1])]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('All params:',self.parameters)\n",
    "        #print('W\\'s transposed:',self.parameters[0].T)\n",
    "        #print('Given x:',x)\n",
    "        #Multiplies all x values by their respective weights, then adds them by row\n",
    "        wXSum = np.matmul(x, self.parameters[0].T)\n",
    "        #print('Applied weights:',wXSum)\n",
    "        #print('Adding b:',wXSum + self.parameters[1])\n",
    "\n",
    "        #Adding up all wx + b values\n",
    "        return wXSum + self.parameters[1]\n",
    "    \n",
    "    def backward(self, grad, original_input):\n",
    "        #Using the chain rule, multiply the incoming gradient with the current gradient\n",
    "        #Storing Partial Derivatives of Y = Xw + b. Multiplied by incoming grad\n",
    "        #dw = original input\n",
    "        self.grads[0] = np.matmul(grad.T, original_input)\n",
    "        #db = 1\n",
    "        self.grads[1] = grad\n",
    "        \n",
    "        #Derivative of linear combination w.r.t the original input\n",
    "        #dy/dx = w\n",
    "        grad_x = np.dot(grad, self.parameters[0]) #Order here matters\n",
    "        return grad_x\n",
    "    \n",
    "    def __call__(self, x, mode=None):\n",
    "        return self.forward(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parameters: [array([[-0.12446163, -0.13230478,  0.32909129,  0.10957762,  0.00897228],\n",
      "       [ 0.24549829, -0.22203597,  0.14655944,  0.13511814, -0.09848324]]), array([-0.15246773,  0.08739301])]\n",
      "[0.92890684 0.37655399]\n"
     ]
    }
   ],
   "source": [
    "#Self testing for understanding\n",
    "'''Each hidden unit generates its own weights the length of our given input dimensions and a bias.\n",
    "Those input dimensions are the number of variables in the given array x. \n",
    "Each value within x is multiplied by the weight of the same index (for all hidden units), then\n",
    "added up to a single value. We then add the bias value to that value.'''\n",
    "x2 = np.array([1, 2, 3, 4, 5])\n",
    "aLayer = AffineLayer(input_dim=5, hidden_units=2)\n",
    "print('Starting parameters:',aLayer.parameters)\n",
    "print(aLayer(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropout layer https://machinelearning.wtf/terms/inverted-dropout/\n",
    "class Dropout(Layers):\n",
    "  #p = keep probability\n",
    "  #layer that keeps weights with probability p. \n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self,x,mode=None):\n",
    "        #We only dropout during training\n",
    "        if mode == 'train':\n",
    "            #Shape of the incoming array, random values between 0-1\n",
    "            if len(x.shape) > 1:\n",
    "              self.mask = np.random.rand(x.shape[0], x.shape[1])\n",
    "            else:\n",
    "              self.mask = np.random.rand(x.shape[0])\n",
    "            #print(\"Randomly generated values as the shape of input: \\n{}\".format(self.mask))\n",
    "\n",
    "            #Convert random value matrix to binary w.r.t keep probability\n",
    "            #Keep values less than p (set to 1), discard those above (set to 0)\n",
    "            self.mask = self.mask < self.p\n",
    "            #print(\"Mask after converting to binary with rule < {}(p value) : \\n{}\".format(self.p, self.mask))\n",
    "\n",
    "            #Multiply neurons by the keep matrix\n",
    "            x = np.multiply(x, self.mask)\n",
    "        \n",
    "            #Scale values by p for inverted dropout during training\n",
    "            #1e-7 to prevent division by 0\n",
    "            x = x / ((1 - self.p) + 1e-7)\n",
    "        #print(\"Dropout forward result:\\n{}\".format(x))\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad,original_input):\n",
    "        #Want to use the same mask as before to shut down the same neurons\n",
    "        #We only use backward pass in training.\n",
    "        grad_x = (grad * self.mask) / ((1 - self.p) + + 1e-7)\n",
    "\n",
    "        #return gradient with respect to the original input to this layer\n",
    "        return grad_x   \n",
    "    \n",
    "    def __call__(self,x,mode='test'):\n",
    "        return self.forward(x,mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Stochastic GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(metaclass=abc.ABCMeta):\n",
    "    @abc.abstractmethod\n",
    "    def step(self, layer):\n",
    "        return\n",
    "    \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, momentum, lr):\n",
    "        #The factor at which the momentum increases each time\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        #Learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        #Momentum seperately for each layer\n",
    "        self.momenDict = {}\n",
    "        \n",
    "    def step(self, layer):\n",
    "        #Storing layers in their dictionary of momentums\n",
    "        if layer not in self.momenDict.keys():\n",
    "            self.momenDict[layer] = {}\n",
    "            #bias momentum\n",
    "            self.momenDict[layer]['bM'] = 0\n",
    "            #weight momentum\n",
    "            self.momenDict[layer]['wM'] = 0\n",
    "            \n",
    "        #The avg of the gradients stored for b\n",
    "        biasGradient = np.sum(layer.grads[1], axis=0) / len(layer.grads[1])\n",
    "        \n",
    "        #Will start off by adding 0, then will build momentum each step\n",
    "        self.momenDict[layer]['bM'] = biasGradient + (self.momenDict[layer]['bM'] * self.momentum)\n",
    "        self.momenDict[layer]['wM'] = layer.grads[0] + (self.momenDict[layer]['wM'] * self.momentum)\n",
    "        \n",
    "        #Implementing the step, in the negative GD direction (descent)\n",
    "        layer.parameters[1] -= self.momenDict[layer]['bM'] * self.lr\n",
    "        layer.parameters[0] -= self.momenDict[layer]['wM'] * self.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layername': {'bM': 0, 'wM': 0}}\n"
     ]
    }
   ],
   "source": [
    "dictT = {}\n",
    "dictT['layername'] = {}\n",
    "dictT['layername']['bM'] = 0\n",
    "            #weight momentum\n",
    "dictT['layername']['wM'] = 0\n",
    "print(dictT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Implementing a FCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Fully Connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "(10000, 784)\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Data Preparation\n",
    "def prepareMNIST(data, targets):\n",
    "    #Data will be a certain amount of images of 28 x 28 pixels\n",
    "    #We want to combine all the pixels into 1 vector\n",
    "    newData = data.reshape(data.shape[0], 784)\n",
    "    \n",
    "    #Next we need to change our targets into one hot vectors of size 10 (the number drawings are from 0-9)\n",
    "    #For all sample indices in the sample range, at the index location of the target place a 1\n",
    "    #numpy allows us to us a range of numbers for indices reference\n",
    "    temp = np.zeros((len(targets), 10))\n",
    "    temp[np.arange(targets.size), targets] = 1\n",
    "    newTargets = temp\n",
    "    \n",
    "    return newData, newTargets\n",
    "\n",
    "#Loading data\n",
    "x_train, y_train = (np.load(\"train.npz\")['arr_0'], np.load(\"train.npz\")['arr_1'])\n",
    "train_data, train_targets = prepareMNIST(x_train, y_train)\n",
    "print(train_data.shape)\n",
    "print(train_targets[:3])\n",
    "\n",
    "x_test, y_test = (np.load(\"test.npz\")['arr_0'], np.load(\"test.npz\")['arr_1'])\n",
    "test_data, test_targets = prepareMNIST(x_test, y_test)\n",
    "print(test_data.shape)\n",
    "print(test_targets[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.original_inputs = []\n",
    "        \n",
    "    def forward(self, x, y=None, mode='test'):\n",
    "        #Storing the initial inputs, keeps each input to each layer in order\n",
    "        self.original_inputs = [x.copy()]\n",
    "\n",
    "        for i in range(0, len(self.layers)):\n",
    "            #print(len(self.layers))\n",
    "            #print(self.layers[i])\n",
    "            #The last layer calculates loss so we hand it y targets.\n",
    "            if i == (len(self.layers) - 1):\n",
    "                #print('HERE', self.layers[i])\n",
    "                self.original_inputs.append(self.layers[i](self.original_inputs[-1], y=y, mode=mode))\n",
    "            else:\n",
    "                #Each layer gets the most recent output as their input (Starts with x added above)\n",
    "                self.original_inputs.append(self.layers[i](self.original_inputs[-1], mode))\n",
    "        \n",
    "        #The last added input/output is our result (loss or prediction)\n",
    "        return self.original_inputs[-1]\n",
    "        \n",
    "    def backward(self):\n",
    "        #Pop the loss value from the end of our inputs\n",
    "        loss = self.original_inputs.pop()\n",
    "        \n",
    "        #Starting from the loss function, L=1 for backprop\n",
    "        grad = 1 \n",
    "        \n",
    "        #Looping backwards through the layers, applying the chain rule\n",
    "        #Python backwards loop range(inclusive start, exclusive end, step)\n",
    "        for i in range(len(self.layers)-1, -1, -1):\n",
    "            #print('Layer:', self.layers[i])\n",
    "            origin = self.original_inputs.pop()\n",
    "            \n",
    "            grad = self.layers[i].backward(grad, origin)\n",
    "            \n",
    "        #Return grad w.r.t input only\n",
    "        return grad\n",
    "    \n",
    "    def __call__(self, x, y=None, mode='test'):\n",
    "        return self.forward(x, y, mode)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#print(myNN.layers)\\nresult = myNN(x=train_data[:testSize], y=train_targets[:testSize], mode='train')\\n#result = myNN(x=train_data[:500], mode='test')\\nprint(len(result), result[:testSize])\\ngrad = myNN.backward()\\nprint(len(grad), grad[:testSize])\\nfor layer in myNN.layers:\\n    if type(layer) == AffineLayer:\\n        optimizer.step(layer)\""
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "momentum = 0.95\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "batchSize = 100\n",
    "testSize = 5\n",
    "myNN = FCNN([AffineLayer(784, 32), Sigmoid(), AffineLayer(32, 10), SoftMaxCrossLoss()])\n",
    "optimizer = SGD(momentum, lr)\n",
    "'''#print(myNN.layers)\n",
    "result = myNN(x=train_data[:testSize], y=train_targets[:testSize], mode='train')\n",
    "#result = myNN(x=train_data[:500], mode='test')\n",
    "print(len(result), result[:testSize])\n",
    "grad = myNN.backward()\n",
    "print(len(grad), grad[:testSize])\n",
    "for layer in myNN.layers:\n",
    "    if type(layer) == AffineLayer:\n",
    "        optimizer.step(layer)'''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Epoch 1/10:0.33487806749640725\n",
      "Loss at Epoch 2/10:0.18647286693915407\n",
      "Loss at Epoch 3/10:0.15550975880677081\n",
      "Loss at Epoch 4/10:0.1419374365230694\n",
      "Loss at Epoch 5/10:0.12860288578149962\n",
      "Loss at Epoch 6/10:0.12173965292708294\n",
      "Loss at Epoch 7/10:0.11681663570312592\n",
      "Loss at Epoch 8/10:0.1068557343492225\n",
      "Loss at Epoch 9/10:0.10598730510460674\n",
      "Loss at Epoch 10/10:0.10116960082058919\n"
     ]
    }
   ],
   "source": [
    "#A history of loss will be stored\n",
    "loss = np.zeros(epochs)\n",
    "\n",
    "#Train for the decided number of Epochs\n",
    "for i in range(epochs):\n",
    "    #print('StartL:', loss)\n",
    "    #For training data randomization\n",
    "    shuffleRange = np.arange(train_data.shape[0])\n",
    "    #Shuffling the order of indices?\n",
    "    np.random.shuffle(shuffleRange)\n",
    "    #print(shuffleRange)\n",
    "    \n",
    "    #print(shuffleRange[i:min(i+batchSize, len(train_data))])\n",
    "    #start:min(batch, len) for 0-len, batch step\n",
    "    batches = [shuffleRange[i:min(i+batchSize, len(train_data))] for i in range(0, train_data.shape[0], batchSize)]\n",
    "    #print(len(batches), batches[:1])\n",
    "    \n",
    "    #Loop through shuffled samples\n",
    "    for batch in batches:\n",
    "        #A list of costs will be returned, we want to store the summed cost for total Losses\n",
    "        loss[i] += np.sum(myNN(train_data[batch], y=train_targets[batch], mode='train'))\n",
    "        #Backprop to calculate and store gradients\n",
    "        myNN.backward()\n",
    "        \n",
    "        #Implement an optimization step for the weights in the affine layer\n",
    "        for layer in myNN.layers:\n",
    "            if type(layer) == AffineLayer:\n",
    "                optimizer.step(layer)\n",
    "\n",
    "    #if i % 25 == 0 :\n",
    "    print(f\"Loss at Epoch {i+1}/{epochs}:{loss[i]/train_data.shape[0]}\")\n",
    "    #print('EndL:', loss)\n",
    "    \n",
    "#Average the costs\n",
    "#print('Total loss before', loss)\n",
    "loss = loss / train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33487807 0.18647287 0.15550976 0.14193744 0.12860289 0.12173965\n",
      " 0.11681664 0.10685573 0.10598731 0.1011696 ]\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#print(epochs)\n",
    "#print(np.arange(0, epochs+1))\n",
    "#print(loss)\n",
    "plt.plot(np.arange(1, epochs+1), loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Evaluation (Accuracy, Precision, Recall)\n",
    "\n",
    "We run our evaluation on our test data set so that we can measure our model's prediction performance on samples it has never seen before.  If we had high accuracy on the training set but not on the test set, that means we have overfit out model to our training data.\n",
    "\n",
    "The three measures we'll use and their application are:\n",
    "\n",
    "Accuracy: How many correct predictions are made by the model overall?\n",
    "\n",
    "Precision: How many times was a guess for a specific number true? (Does it struggle with some numbers?)\n",
    "\n",
    "Recall: How often could our model correctly identify a specific number when it saw it?\n",
    "\n",
    "These can be calculated using True, False, Positive and Negative predictions.  \n",
    "In the case of our multiclass classification we have to sum up all the possible predictions when considering each individual class.\n",
    "\n",
    "For example, the classification of the number 2 we need to consider:\n",
    "True Positives = Our model guessed 2 and the target was 2\n",
    "True Negatives = Our model guessed *any number other than 2* and the target was indeed that number (Aka all correct predictions of numbers other than 2)\n",
    "*True Negatives are actually not important to most of our calculations except when we sum all predictions, so we won't be needing to calculate it directly. They are just conceptually important to consider*\n",
    "False Positives = Our model guessed 2 and the target was *any number other than 2*\n",
    "False Negative = Our model guessed *any number other than 2* and the target was 2\n",
    "\n",
    "This needs to be done for all 0-9 classes. \n",
    "With the values, we can calculate:\n",
    "\n",
    "Accuracy = $TP + TN / TP + TN + FP + FN$\n",
    "Precision = $TP / TP + FP$\n",
    "Recall = $TP / TP + FN$\n",
    "\n",
    "The cleanest way of doing this is with a confusion matrix, which will have the dimensions 10x10 to store cases between each of the 10 classes to eachother. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running our neural network on the test data\n",
    "testResult = myNN(x=test_data, mode='test')\n",
    "\n",
    "#Converting the return probabilities to their predictions (the index of the highest %)\n",
    "predictions = np.argmax(testResult, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0\t1\t2\t3\t4\t5\t6\t7\t8\t9 \n",
      "\n",
      "0\t 956\t0\t6\t2\t1\t8\t4\t0\t7\t6\n",
      "1\t 1\t1117\t0\t1\t0\t2\t3\t11\t2\t2\n",
      "2\t 4\t4\t981\t10\t3\t1\t1\t14\t6\t1\n",
      "3\t 0\t4\t14\t968\t3\t21\t2\t1\t7\t7\n",
      "4\t 0\t0\t7\t2\t943\t3\t7\t4\t8\t15\n",
      "5\t 4\t1\t4\t6\t1\t819\t7\t2\t4\t1\n",
      "6\t 6\t1\t2\t1\t4\t13\t924\t0\t7\t1\n",
      "7\t 3\t1\t5\t6\t2\t5\t2\t981\t5\t9\n",
      "8\t 4\t4\t11\t6\t4\t15\t8\t4\t923\t5\n",
      "9\t 2\t3\t2\t8\t21\t5\t0\t11\t5\t962\n"
     ]
    }
   ],
   "source": [
    "#Matrix initialization for 10 x 10 classes\n",
    "nclasses = 10\n",
    "confusionM = np.zeros((nclasses,nclasses), dtype=np.int32)\n",
    "\n",
    "#Counting up predictions where row is the guess, column is the target\n",
    "for i in range(0, len(predictions)):\n",
    "    confusionM[predictions[i]][y_test[i]] += 1\n",
    "    \n",
    "#Rows(left) correspond to the model's guess, columns(top) are the target response\n",
    "labels = list(range(0,nclasses))\n",
    "print(\"\\t\",\"\\t\".join(str(e) for e in labels),\"\\n\")\n",
    "for i in range(0, len(confusionM)):\n",
    "    print(f\"{labels[i]}\\t\", '\\t'.join(str(e) for e in confusionM[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: [ 956 1117  981  968  943  819  924  981  923  962]\n",
      "FP: [34 22 44 59 46 30 35 38 61 57]\n",
      "FN: [24 18 51 42 39 73 34 47 51 47]\n",
      "Overall(avg) Accuracy: 0.9572401474611283\n",
      "\n",
      "Class\tPrecision\tRecall\n",
      "0:\t96.57%\t\t97.55%\n",
      "1:\t98.07%\t\t98.41%\n",
      "2:\t95.71%\t\t95.06%\n",
      "3:\t94.26%\t\t95.84%\n",
      "4:\t95.35%\t\t96.03%\n",
      "5:\t96.47%\t\t91.82%\n",
      "6:\t96.35%\t\t96.45%\n",
      "7:\t96.27%\t\t95.43%\n",
      "8:\t93.80%\t\t94.76%\n",
      "9:\t94.41%\t\t95.34%\n"
     ]
    }
   ],
   "source": [
    "'''Functions to calculate the predictions\n",
    "Accuracy = TP + TN / TP + TN + FP + FN (all results)\n",
    "Precision = TP / TP + FP\n",
    "Recall = TP / TP + FN'''\n",
    "\n",
    "#The middle diagonal value of the matrix is where the prediction and target meet (correct guess)\n",
    "def truePos(cFM):\n",
    "    return np.diagonal(cFM)\n",
    "\n",
    "#\n",
    "def falsePos(cFM):\n",
    "    #Summing by columns because rows represent the guesses per class\n",
    "    return np.sum(cFM, axis=1) - truePos(cFM)\n",
    "\n",
    "#\n",
    "def falseNeg(cFM):\n",
    "    #Now we sum over the rows (prediction), minus the correct guesses. This sums up all incorrect guesses per column (target)\n",
    "    return np.sum(cFM, axis=0) - truePos(cFM)\n",
    "\n",
    "TP = truePos(confusionM)\n",
    "FP = falsePos(confusionM)\n",
    "FN = falseNeg(confusionM)\n",
    "print('TP:',TP)\n",
    "print('FP:',FP)\n",
    "print('FN:',FN)\n",
    "\n",
    "#Accuracy per class\n",
    "accuracies = TP / np.sum(confusionM, axis=1)\n",
    "precisions = TP / (TP + FP)\n",
    "recalls = TP / (TP + FN)\n",
    "\n",
    "print('Overall(avg) Accuracy:',np.average(accuracies))\n",
    "#Precision and Recall are meant to measure class specific performance, so averaging is not useful to us\n",
    "print('\\nClass\\tPrecision\\tRecall')\n",
    "for i in range(0, len(labels)):\n",
    "    print(\"%d:\\t%.2f%%\\t\\t%.2f%%\" % (labels[i], precisions[i]*100, recalls[i]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.9996161 0.000015  0.0000002 0.0000019 0.        0.0000001\n",
      " 0.0003665 0.0000001 0.       ]\n",
      "Target value: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x_test, y_test\n",
    "\n",
    "def printDIGIT(data, target, index):\n",
    "    plt.imshow(data[index], cmap=plt.get_cmap('gray'))\n",
    "    print('Target value:', target[index])\n",
    "\n",
    "\n",
    "print(np.array_str(testResult[0], precision=7, suppress_small=True))\n",
    "printDIGIT(x_test, y_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 1 ... 1 4 8]\n",
      "[1 5 1 ... 1 4 8]\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax(testResult, axis=1)\n",
    "print(pred)\n",
    "print(y_test)\n",
    "\n",
    "different = []\n",
    "for i in range(0, len(y_test)):\n",
    "    if pred[i] != y_test[i]:\n",
    "        different.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9574\n",
      "Incorrectly classified numbers:  426\n",
      "Error found at index 652. Pred:8 Target:5\n",
      "Target value: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Accuracy:', 1 - len(different)/len(y_test))\n",
    "print('Incorrectly classified numbers: ', len(different))\n",
    "errI = 20\n",
    "print(f\"Error found at index {different[errI]}. Pred:{pred[different[errI]]} Target:{y_test[different[errI]]}\")\n",
    "printDIGIT(x_test, y_test, different[errI])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional References used:\n",
    "\n",
    "    *https://shiffdag.medium.com/what-is-accuracy-precision-and-recall-and-why-are-they-important-ebfcb5a10df2\n",
    "    *https://www.mage.ai/blog/definitive-guide-to-accuracy-precision-recall-for-product-developers\n",
    "    *https://towardsdatascience.com/precision-and-recall-88a3776c8007#:~:text=Precision%20is%20calculated%20by%20dividing,was%20predicted%20as%20a%20positive.&text=Recall%20(or%20True%20Positive%20Rate,have%20been%20predicted%20as%20positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Garbage heap\n",
    "''' Maybe try to print with labels at the end so as to not mess up the math??\n",
    "#Adding the class labels for clarity. -1 in the top left is NOT a class, just a blank space in the matrix.\n",
    "nclasses = 10\n",
    "confusionM = np.zeros((nclasses+1,nclasses+1))\n",
    "for i in range(0,nclasses+1):\n",
    "    confusionM[i][0] = i-1\n",
    "    confusionM[0][i] = i-1\n",
    "#Counting up predictions where row is the guess, column is the target\n",
    "#Since we included class labels, we push the indices of the targets + 1\n",
    "for i in range(0, len(predictions)):\n",
    "    confusionM[predictions[i]+1][y_test[i]+1] += 1'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
